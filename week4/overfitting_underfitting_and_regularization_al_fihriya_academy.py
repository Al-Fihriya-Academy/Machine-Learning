# -*- coding: utf-8 -*-
"""Overfitting/Underfitting and Regularization - Al Fihriya Academy

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JBS6uu7TddRUUhhzxxcnrqznXaFw71V-
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
# Plotting.
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
plt.rcParams['figure.figsize'] = (10, 6)
# %config InlineBackend.figure_formats = ['svg']

"""# Fitting a polynomial model."""

#@title We generate datapoints from a polynomial function.
n_points = 10
noise_magnitude = 0.3

x = np.linspace(0.0, 1.0, n_points)
x.sort()

y_real = 1.5 * x ** 2.1 - 0.3

# We add some artificial noise, as it is quite common to train on noisy data.
np.random.seed(1)
y = y_real + noise_magnitude * np.random.normal(size=n_points)

plt.plot(x, y_real, '--', label='Real function')
plt.scatter(x, y, s=100, c='g', label='Data points observed')
_ = plt.legend(loc='best')

def to_polynomial(x):
  return np.vstack((x, x**2, x**3, x**4, x**5, x**6, x**7, x**8)).T
X = to_polynomial(x)

from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV
lr = LinearRegression()
lasso = LassoCV(max_iter=50000) # L1 regularization
ridge = RidgeCV() # L2 regularization

lr.fit(X, y)
lasso.fit(X, y)
ridge.fit(X, y)

lr.coef_.astype(str)

lasso.coef_

ridge.coef_

x_pred = np.linspace(0, 1, 100)
X_pred = to_polynomial(x_pred)

#plt.plot(x, y_real, '--', alpha=0.5, label='Real function')
plt.scatter(x, y, s=100, c='g', label='Data points')
plt.plot(x_pred, lr.predict(X_pred), c='red', label='Prediction - No regularization')
plt.plot(x_pred, lasso.predict(X_pred), c='blue',  alpha=0.5, label='Prediction - L1 reguralization')
plt.plot(x_pred, ridge.predict(X_pred), c='purple', alpha=0.5,  label='Prediction - L2 reguralization')
_ = plt.legend(loc='best')

"""# Impact of different levels of regularization."""

from sklearn.linear_model import Ridge
import seaborn as sns

x_pred = np.linspace(0, 1, 100)
X_pred = to_polynomial(x_pred)

plt.scatter(x, y, s=100, c='g', label='Data points')
for i, alpha in enumerate([1.0, 0.01, 0.0001, 0.]):
  ridge = Ridge(alpha=alpha)
  ridge.fit(X, y)
  plt.plot(x_pred, ridge.predict(X_pred), '--', c=sns.color_palette('hsv')[i], alpha=0.8, linewidth=2, label=f'Prediction - lambda={alpha}')
_ = plt.legend(loc='best')